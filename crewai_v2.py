# -*- coding: utf-8 -*-
"""CrewAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18L1BEOp4WX2aDIJQWxhzIwYAvShEdiZI
"""

!sudo apt-get update
!pip install crewai
!pip install 'crewai[tools]'
!pip install google-api-python-client
!sudo apt-get update
!pip install crewai crewai-tools google-api-python-client langchain-openai databricks-sdk

!pip install langchain-google-genai

from crewai import Agent, Task, Crew, Process
from crewai_tools import ScrapeWebsiteTool
from googleapiclient.discovery import build
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.tools import BaseTool, Tool
from langchain.tools import Tool
import os
from google.colab import userdata

try:
    from crewai_tools import ScrapeWebsiteTool
except ImportError as e:
    print(f"Warning: {e}. Falling back to basic scraping.")
    class ScrapeWebsiteTool:
        def __init__(self):
            self.name = "ScrapeWebsiteTool"
            self.description = "Basic web scraping functionality"

        def scrape(self, url):
            import requests
            from bs4 import BeautifulSoup
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            return soup.get_text()

import os
from google.colab import userdata

os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
os.environ["GOOGLE_CSE_ID"] = userdata.get('GOOGLE_CSE_ID')

from googleapiclient.discovery import build
from langchain.tools import Tool
import os

# --- Define Google Search Function ---
class GoogleSearchTool:
    def __init__(self):
        self.api_key = os.getenv("GOOGLE_API_KEY")
        self.cse_id = os.getenv("GOOGLE_CSE_ID")
        self.service = build("customsearch", "v1", developerKey=self.api_key)

    def run(self, query: str) -> str:
        result = self.service.cse().list(
            q=query,
            cx=self.cse_id,
            num=5
        ).execute()
        links = "\n".join([item['link'] for item in result.get('items', [])])
        return links



# --- Initialize Tools ---
google_search_tool = GoogleSearchTool()
scrape_tool = ScrapeWebsiteTool()

google_search_tool = Tool(
    name="Google Search Tool",
    description="Search Google for relevant information.",
    func=GoogleSearchTool().run
)

from crewai_tools import ScrapeWebsiteTool

# Directly use it without wrapping
scrape_tool = ScrapeWebsiteTool()

# Set the Topic Here
#######
topic = """
Look for the latest Tech and AI trends in 2025.
"""
####

from crewai import Agent
from langchain_google_genai import ChatGoogleGenerativeAI

researcher = Agent(
    role='Principal AI Researcher',
    goal='Uncover cutting-edge developments in AI and technology',
    backstory="""You are an expert researcher...""",
    verbose=True,
    allow_delegation=False,
    tools=[google_search_tool, scrape_tool],  # âœ… both valid now
    llm=ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.5)
)


# --- Define Writer Agent ---
writer = Agent(
    role='Senior Tech Writer',
    goal='Craft engaging and insightful blogs on technology trends',
    backstory="""You are a talented tech writer with a flair for making complex AI concepts
    accessible and engaging to a broad audience.""",
    verbose=True,
    allow_delegation=False,
    tools=[scrape_tool],
    llm=ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.5)
)

# Define tasks
research = Task(
    description="""
      Extract key insights, ideas, and information from AI topics
      related to technology and self-improvement.
    """,
    expected_output="""
      A concise report on AI and technology, containing key insights
      and recommendations in bullet points.
    """,
    agent=researcher,
    output_file="researcher_tasks.md"
)

write_blog = Task(
    description="""
    Write an engaging blog post based on the research on AI advancements.
    """,
    expected_output="""
      A full blog post of around 500 words with citations from all the URLs.
    """,
    agent=writer,
    output_file="writer_tasks.md"
)

# Instantiate and kickoff the crew
crew = Crew(
    agents=[researcher, writer],
    tasks=[research, write_blog],
    verbose=True,
    process=Process.sequential  # Use parallel if no dependencies between tasks
)

# Kickoff the crew
result = crew.kickoff()